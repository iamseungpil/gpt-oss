#!/usr/bin/env python3
"""
GPT-OSS ARC Training with TRL GRPO + MoE LoRA Fix
=====================================================

Fixed LoRA target modules to include MoE expert layers.
"""

import os
import sys
import json
import numpy as np
import torch
from datetime import datetime
import logging
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import re

# Environment setup
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_VISIBLE_DEVICES"] = "6"
os.environ["WANDB_API_KEY"] = "2f4e627868f1f9dad10bcb1a14fbf96817e6baa9"
# Memory optimization
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Core imports
import wandb
from arc import train_problems
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments
)
from trl import GRPOConfig, GRPOTrainer
from peft import LoraConfig, get_peft_model, TaskType

# Configuration
DATA_DIR = Path("/data/gpt_oss_final")
LOG_FILE = DATA_DIR / "logs" / "grpo_moe_lora.log"
RESPONSE_DIR = DATA_DIR / "logs" / "grpo_moe_responses"

# Setup logging
LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
RESPONSE_DIR.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def parse_grid_from_response(response: str, target_shape: Tuple[int, int]) -> Optional[np.ndarray]:
    """Extract grid from model response"""
    
    # Try to find grid in code blocks
    code_block_pattern = r'```(?:python|text|)?\\n?([\\s\\S]*?)```'
    code_blocks = re.findall(code_block_pattern, response, re.MULTILINE)
    
    for block in code_blocks:
        lines = block.strip().split('\\n')
        grid = []
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            # Parse numbers from line
            numbers = []
            for item in line.replace(',', ' ').split():
                try:
                    num = int(item)
                    numbers.append(num)
                except ValueError:
                    continue
            
            if numbers:
                grid.append(numbers)
        
        # Check if valid grid
        if grid and len(grid) == target_shape[0]:
            if all(len(row) == target_shape[1] for row in grid):
                return np.array(grid)
    
    return None

def grid_to_string(grid: np.ndarray) -> str:
    """Convert numpy grid to string format"""
    return '\\n'.join(' '.join(str(int(cell)) for cell in row) for row in grid)

def compute_arc_reward(response: str, target_grid: np.ndarray) -> float:
    """Compute reward for ARC task"""
    
    parsed_grid = parse_grid_from_response(response, target_grid.shape)
    
    if parsed_grid is None:
        return -1.0
    
    # Perfect match
    if np.array_equal(parsed_grid, target_grid):
        return 1.0
    
    # Partial credit for correct cells
    correct_cells = np.sum(parsed_grid == target_grid)
    total_cells = target_grid.size
    accuracy = correct_cells / total_cells
    
    # Scale reward
    if accuracy > 0.9:
        return 0.5
    elif accuracy > 0.7:
        return 0.2
    else:
        return -0.5

def create_arc_prompt(problem) -> str:
    """Create ARC prompt with Harmony format"""
    
    # Build examples section
    examples = []
    for i, train_pair in enumerate(problem.train_pairs, 1):
        examples.append(f"Example {i}: Input:\\n{grid_to_string(train_pair.x)}\\nOutput:\\n{grid_to_string(train_pair.y)}\\n")
    
    # Test section
    test_input = problem.test_pairs[0].x
    examples_text = '\\n'.join(examples)
    
    # With proper Harmony format
    prompt = f"""<|start|>system<|message|>You are a pattern solver. Use high reasoning.
Reasoning: high
Output format: Provide the final answer as a grid of numbers.<|end|>
<|start|>user<|message|>Solve this pattern and output the grid:
{examples_text}
Test Input:
{grid_to_string(test_input)}
Test Output:<|end|>
<|start|>assistant<|message|>"""
    
    return prompt

def main_grpo_moe_lora():
    """Main GRPO training with MoE LoRA fix"""
    
    logger.info("üöÄ Starting GPT-OSS ARC GRPO Training (MoE LoRA Fix)")
    logger.info("="*60)
    
    try:
        # Setup WandB
        wandb.login(key="2f4e627868f1f9dad10bcb1a14fbf96817e6baa9")
        wandb.init(
            project="gpt-oss-arc-grpo-moe-lora",
            name=f"moe_lora_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        )
        
        # Load model and tokenizer with BitsAndBytesConfig
        logger.info("Loading model and tokenizer...")
        
        # 4-bit quantization config
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            "unsloth/gpt-oss-20b-BF16", 
            trust_remote_code=True,
            padding_side="left",
        )
        
        # Add pad token if missing
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        
        # Load model
        model = AutoModelForCausalLM.from_pretrained(
            "unsloth/gpt-oss-20b-BF16",
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )
        
        # üéØ **BREAKTHROUGH FIX**: Add MoE Expert layers to LoRA targets!
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            # üîß FIXED: Include MoE expert projection layers!
            target_modules=[
                # Attention layers
                "q_proj", "k_proj", "v_proj", "o_proj",
                # üöÄ MoE Expert layers (THE MISSING PIECE!)
                "gate_up_proj", "down_proj"
            ],
        )
        
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        
        # Memory optimization
        torch.cuda.empty_cache()
        
        logger.info("‚úÖ Model and tokenizer loaded successfully with MoE LoRA")
        
        # Prepare dataset - Start small
        problems = train_problems[:1]  # Single problem for testing
        logger.info(f"üìä Loaded {len(problems)} ARC problems")
        
        # Create prompts
        prompts = []
        targets = []
        
        for problem in problems:
            prompt = create_arc_prompt(problem)
            prompts.append(prompt)
            targets.append(problem.test_pairs[0].y)
        
        # Create dataset
        dataset_dict = {
            "prompt": prompts,  # GRPO expects 'prompt' field
            "problem_id": [p.uid for p in problems]
        }
        
        train_dataset = Dataset.from_dict(dataset_dict)
        logger.info(f"üìä Created dataset with {len(train_dataset)} examples")
        
        # GRPO Config - Conservative settings
        grpo_config = GRPOConfig(
            output_dir=str(DATA_DIR / "checkpoints_grpo_moe"),
            learning_rate=1e-5,
            num_train_epochs=1,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=1,
            warmup_steps=2,
            logging_steps=1,
            save_steps=5,
            bf16=True,
            fp16=False,
            optim="adamw_8bit",
            report_to="wandb",
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            # GRPO specific - Conservative
            num_iterations=1,
            epsilon=0.2,
            epsilon_high=0.28,
            beta=0.0,
            loss_type="dapo",
            max_prompt_length=512,
            max_completion_length=256,
            num_generations=2,
            generation_batch_size=2,
            temperature=0.7,
            top_p=0.9,
            max_steps=5,  # Very small for testing
        )
        
        # Custom reward function - flexible parameter handling
        def reward_function(*args, **kwargs) -> List[float]:
            """Compute rewards for GRPO - flexible parameter handling"""
            
            # Extract responses from args/kwargs
            responses = None
            if 'responses' in kwargs:
                responses = kwargs['responses']
            elif 'completions' in kwargs:
                responses = kwargs['completions']
            elif len(args) >= 3:
                responses = args[2]
            elif len(args) >= 1:
                responses = args[0]
            
            if responses is None:
                logger.error("Could not find responses in reward function arguments")
                return [0.0]
            
            rewards = []
            logger.info(f"Computing rewards for {len(responses)} responses")
            
            for i, response in enumerate(responses):
                # Get corresponding target
                target_idx = i % len(targets)
                target_grid = targets[target_idx]
                
                # Compute reward
                reward = compute_arc_reward(response, target_grid)
                rewards.append(reward)
                
                logger.info(f"Response {i}: reward = {reward:.2f}")
            
            return rewards
        
        # Create trainer - TRL GRPOTrainer interface
        trainer = GRPOTrainer(
            model=model,
            reward_funcs=reward_function,
            args=grpo_config,
            train_dataset=train_dataset,
            processing_class=tokenizer,
        )
        
        logger.info("üöÄ Starting GRPO training with MoE LoRA fix...")
        
        # Train
        trainer.train()
        
        logger.info("‚úÖ Training completed!")
        
        # Save model
        model.save_pretrained(DATA_DIR / "final_model_grpo_moe_lora")
        tokenizer.save_pretrained(DATA_DIR / "final_model_grpo_moe_lora")
        logger.info(f"üíæ Model saved to {DATA_DIR / 'final_model_grpo_moe_lora'}")
        
    except Exception as e:
        logger.error(f"‚ùå Training error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        wandb.finish()

if __name__ == "__main__":
    main_grpo_moe_lora()