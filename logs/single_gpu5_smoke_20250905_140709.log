/home/ubuntu/miniconda3/envs/gpt-oss/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2025-09-05 14:07:14,965 | INFO | ================================================================================
2025-09-05 14:07:14,965 | INFO | ðŸš€ Starting GPT-OSS ARC Continual Learning with DAPO v2
2025-09-05 14:07:14,965 | INFO | ðŸ“Š 30,000 token support + Sequential problem training
2025-09-05 14:07:14,965 | INFO | ================================================================================
wandb: Currently logged in as: dbsgh797210 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.3
wandb: Run data is saved locally in /home/ubuntu/gpt-oss/wandb/run-20250905_140715-8jvf91yu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hf_trl_dapo_v2_20250905_140714
wandb: â­ï¸ View project at https://wandb.ai/dbsgh797210/gpt-oss-arc-dapo-v2
wandb: ðŸš€ View run at https://wandb.ai/dbsgh797210/gpt-oss-arc-dapo-v2/runs/8jvf91yu
2025-09-05 14:07:16,355 | INFO | ðŸ“¦ Loading model: openai/gpt-oss-20b
`torch_dtype` is deprecated! Use `dtype` instead!
MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16
2025-09-05 14:07:18,487 | INFO | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:02,  1.25s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:02<00:01,  1.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.17s/it]
2025-09-05 14:07:23,443 | INFO | ðŸ“Š Loaded 1 problems for continual learning
2025-09-05 14:07:23,444 | INFO | ============================================================
2025-09-05 14:07:23,444 | INFO | ðŸŽ¯ CONTINUAL LEARNING: Problem 1/10
2025-09-05 14:07:23,444 | INFO | ðŸ“‹ Problem UID: 6d75e8bb
2025-09-05 14:07:23,444 | INFO | ============================================================
2025-09-05 14:07:24,178 | INFO | ðŸŽ¯ Starting training for problem 1...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998, 'pad_token_id': 200002}.
trainable params: 7,962,624 || all params: 20,922,719,808 || trainable%: 0.0381
  0%|          | 0/1 [00:00<?, ?it/s]