2025-09-02 04:53:04,758 | INFO | ðŸ“¦ Loading model: openai/gpt-oss-20b
`torch_dtype` is deprecated! Use `dtype` instead!
Traceback (most recent call last):
  File "/home/ubuntu/gpt-oss/main_hf_trl_dapo_v2.py", line 515, in <module>
    main()
    ~~~~^^
  File "/home/ubuntu/gpt-oss/main_hf_trl_dapo_v2.py", line 350, in main
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
    ...<3 lines>...
        torch_dtype=torch.bfloat16,
    )
  File "/home/ubuntu/miniconda3/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/ubuntu/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py", line 288, in _wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py", line 5008, in from_pretrained
    hf_quantizer, config, dtype, device_map = get_hf_quantizer(
                                              ~~~~~~~~~~~~~~~~^
        config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/ubuntu/miniconda3/lib/python3.13/site-packages/transformers/quantizers/auto.py", line 305, in get_hf_quantizer
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        config.quantization_config, quantization_config
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/ubuntu/miniconda3/lib/python3.13/site-packages/transformers/quantizers/auto.py", line 220, in merge_quantization_configs
    raise ValueError(
    ...<2 lines>...
    )
ValueError: The model is quantized with Mxfp4Config but you are passing a BitsAndBytesConfig config. Please make sure to pass the same quantization config class to `from_pretrained` with different loading attributes.
