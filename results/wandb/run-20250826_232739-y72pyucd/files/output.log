2025-08-26 23:28:03,554 | INFO | Loading model and tokenizer...
Using MXFP4 quantized models requires a GPU, we will default to dequantizing the model to bf16
2025-08-26 23:28:15,997 | INFO | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00, 10.53s/it]
