2025-08-31 12:25:46,781 | INFO | Loading model and tokenizer...
MXFP4 quantization requires triton >= 3.4.0 and triton_kernels installed, we will default to dequantizing the model to bf16
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:21<00:00,  7.08s/it]
trainable params: 7,962,624 || all params: 20,922,719,808 || trainable%: 0.0381
2025-08-31 12:26:47,293 | INFO | âœ… Model and tokenizer loaded successfully
2025-08-31 12:26:47,294 | INFO | ðŸ“Š Loaded 5 ARC problems
2025-08-31 12:26:47,302 | INFO | ðŸ“Š Created dataset with 5 examples
[2025-08-31 12:26:47,787] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-08-31 12:26:48,104 | INFO | gcc -pthread -B /data/miniforge3/envs/gptoss/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/miniforge3/envs/gptoss/include -fPIC -O2 -isystem /data/miniforge3/envs/gptoss/include -fPIC -c /data/tmp/tmpx4_6o_6j/test.c -o /data/tmp/tmpx4_6o_6j/test.o
2025-08-31 12:26:48,137 | INFO | gcc -pthread -B /data/miniforge3/envs/gptoss/compiler_compat /data/tmp/tmpx4_6o_6j/test.o -laio -o /data/tmp/tmpx4_6o_6j/a.out
2025-08-31 12:26:48,788 | INFO | gcc -pthread -B /data/miniforge3/envs/gptoss/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/miniforge3/envs/gptoss/include -fPIC -O2 -isystem /data/miniforge3/envs/gptoss/include -fPIC -c /data/tmp/tmp5cocw1lp/test.c -o /data/tmp/tmp5cocw1lp/test.o
2025-08-31 12:26:48,821 | INFO | gcc -pthread -B /data/miniforge3/envs/gptoss/compiler_compat /data/tmp/tmp5cocw1lp/test.o -L/usr/local/cuda-12.6 -L/usr/local/cuda-12.6/lib64 -lcufile -o /data/tmp/tmp5cocw1lp/a.out
2025-08-31 12:26:48,918 | INFO | gcc -pthread -B /data/miniforge3/envs/gptoss/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/miniforge3/envs/gptoss/include -fPIC -O2 -isystem /data/miniforge3/envs/gptoss/include -fPIC -c /data/tmp/tmpsrzjsbkp/test.c -o /data/tmp/tmpsrzjsbkp/test.o
2025-08-31 12:26:48,948 | INFO | gcc -pthread -B /data/miniforge3/envs/gptoss/compiler_compat /data/tmp/tmpsrzjsbkp/test.o -laio -o /data/tmp/tmpsrzjsbkp/a.out
[2025-08-31 12:26:52,589] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
2025-08-31 12:26:52,655 | INFO | ðŸš€ Starting HuggingFace TRL DAPO training...
2025-08-31 12:26:52,655 | INFO | ðŸ“ Using corrected Harmony format - no examples in input prompts
2025-08-31 12:26:52,655 | INFO |    Loss type: bnpo
2025-08-31 12:26:52,655 | INFO |    Epsilon: 0.2
2025-08-31 12:26:52,656 | INFO |    Epsilon high: 0.28
2025-08-31 12:26:52,656 | INFO |    Repetition penalty: 1.0
2025-08-31 12:26:52,656 | INFO |    Reward components: 4 (format + size + pixel + final_channel)
  0%|          | 0/100 [00:00<?, ?it/s]/data/miniforge3/envs/gptoss/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
2025-08-31 12:36:23,704 | INFO | Computing rewards for 2 completions at step 0
2025-08-31 12:36:23,705 | INFO | Completion 0: format=-0.100, size=0.000, pixel=0.000, final_penalty=0.050, total=-0.150
2025-08-31 12:36:23,705 | INFO | Completion 1: format=-0.100, size=0.000, pixel=0.000, final_penalty=0.050, total=-0.150
2025-08-31 12:36:23,706 | INFO | Step 0 reward summary: avg=-0.1500Â±0.0000, range=[-0.150, -0.150], grids_found=0/2, perfect=0
2025-08-31 12:36:23,710 | INFO | Saved step 0 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00000.json
  2%|â–         | 2/100 [09:52<6:41:13, 245.65s/it]2025-08-31 12:46:01,939 | INFO | Computing rewards for 2 completions at step 1
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'num_tokens': 5120.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.15000000596046448, 'rewards/reward_function/std': 0.0, 'reward': -0.15000000596046448, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.6181885004043579, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.1}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.0000000000000003e-06, 'entropy': 0.792273998260498, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.2}
2025-08-31 12:46:01,940 | INFO | Completion 0: format=-0.110, size=0.000, pixel=0.000, final_penalty=0.060, total=-0.170
2025-08-31 12:46:01,940 | INFO | Completion 1: format=-0.110, size=0.000, pixel=0.000, final_penalty=0.060, total=-0.170
2025-08-31 12:46:01,940 | INFO | Step 1 reward summary: avg=-0.1700Â±0.0000, range=[-0.170, -0.170], grids_found=0/2, perfect=0
2025-08-31 12:46:01,944 | INFO | Saved step 1 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00001.json
  4%|â–         | 4/100 [19:20<6:21:01, 238.14s/it]2025-08-31 12:55:15,975 | INFO | Computing rewards for 2 completions at step 2
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.000000000000001e-06, 'num_tokens': 10240.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.17000000178813934, 'rewards/reward_function/std': 0.0, 'reward': -0.17000000178813934, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9604941010475159, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.3}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6e-06, 'entropy': 0.9763933420181274, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.4}
2025-08-31 12:55:16,006 | INFO | Completion 0: format=-0.120, size=0.000, pixel=0.000, final_penalty=0.070, total=-0.190
2025-08-31 12:55:16,006 | INFO | Completion 1: format=-0.120, size=0.000, pixel=0.000, final_penalty=0.070, total=-0.190
2025-08-31 12:55:16,006 | INFO | Step 2 reward summary: avg=-0.1900Â±0.0000, range=[-0.190, -0.190], grids_found=0/2, perfect=0
2025-08-31 12:55:16,011 | INFO | Saved step 2 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00002.json
  6%|â–Œ         | 6/100 [28:32<6:04:24, 232.60s/it]2025-08-31 13:04:30,660 | INFO | Computing rewards for 2 completions at step 3
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.000000000000001e-06, 'num_tokens': 15256.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.1899999976158142, 'rewards/reward_function/std': 0.0, 'reward': -0.1899999976158142, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.7705769538879395, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.5}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1e-05, 'entropy': 0.7460969686508179, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.6}
2025-08-31 13:04:30,685 | INFO | Completion 0: format=-0.130, size=0.000, pixel=0.000, final_penalty=0.080, total=-0.210
2025-08-31 13:04:30,686 | INFO | Completion 1: format=-0.130, size=0.000, pixel=0.000, final_penalty=0.080, total=-0.210
2025-08-31 13:04:30,686 | INFO | Step 3 reward summary: avg=-0.2100Â±0.0000, range=[-0.210, -0.210], grids_found=0/2, perfect=0
2025-08-31 13:04:30,709 | INFO | Saved step 3 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00003.json
  8%|â–Š         | 8/100 [37:47<5:53:40, 230.66s/it]2025-08-31 13:14:04,568 | INFO | Computing rewards for 2 completions at step 4
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.894736842105264e-06, 'num_tokens': 20376.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.20999999344348907, 'rewards/reward_function/std': 0.0, 'reward': -0.20999999344348907, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.095283031463623, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.7}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.789473684210527e-06, 'entropy': 0.7031035423278809, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.8}
2025-08-31 13:14:04,569 | INFO | Completion 0: format=-0.140, size=0.000, pixel=0.000, final_penalty=0.090, total=-0.230
2025-08-31 13:14:04,569 | INFO | Completion 1: format=-0.140, size=0.000, pixel=0.000, final_penalty=0.090, total=-0.230
2025-08-31 13:14:04,569 | INFO | Step 4 reward summary: avg=-0.2300Â±0.0000, range=[-0.230, -0.230], grids_found=0/2, perfect=0
2025-08-31 13:14:04,648 | INFO | Saved step 4 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00004.json
 10%|â–ˆ         | 10/100 [47:21<5:51:06, 234.08s/it]/data/miniforge3/envs/gptoss/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.68421052631579e-06, 'num_tokens': 24831.0, 'completions/mean_length': 1715.5, 'completions/min_length': 1383.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 1383.0, 'completions/min_terminated_length': 1383.0, 'completions/max_terminated_length': 1383.0, 'rewards/reward_function/mean': -0.23000000417232513, 'rewards/reward_function/std': 0.0, 'reward': -0.23000000417232513, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9986732602119446, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.9}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.578947368421054e-06, 'entropy': 0.7009371519088745, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.0}
  warnings.warn(
2025-08-31 13:23:26,896 | INFO | Computing rewards for 2 completions at step 5
2025-08-31 13:23:26,896 | INFO | Completion 0: format=-0.150, size=0.000, pixel=0.000, final_penalty=0.100, total=-0.250
2025-08-31 13:23:26,897 | INFO | Completion 1: format=-0.150, size=0.000, pixel=0.000, final_penalty=0.100, total=-0.250
2025-08-31 13:23:26,897 | INFO | Step 5 reward summary: avg=-0.2500Â±0.0000, range=[-0.250, -0.250], grids_found=0/2, perfect=0
2025-08-31 13:23:26,967 | INFO | Saved step 5 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00005.json
 12%|â–ˆâ–        | 12/100 [56:43<5:41:59, 233.17s/it]2025-08-31 13:32:45,479 | INFO | Computing rewards for 2 completions at step 6
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.473684210526315e-06, 'num_tokens': 29951.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.25, 'rewards/reward_function/std': 0.0, 'reward': -0.25, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.8006758689880371, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.1}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.36842105263158e-06, 'entropy': 1.1605432033538818, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.2}
2025-08-31 13:32:45,480 | INFO | Completion 0: format=-0.160, size=0.000, pixel=0.000, final_penalty=0.110, total=-0.270
2025-08-31 13:32:45,480 | INFO | Completion 1: format=-0.160, size=0.000, pixel=0.000, final_penalty=0.110, total=-0.270
2025-08-31 13:32:45,481 | INFO | Step 6 reward summary: avg=-0.2700Â±0.0000, range=[-0.270, -0.270], grids_found=0/2, perfect=0
2025-08-31 13:32:45,485 | INFO | Saved step 6 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00006.json
 14%|â–ˆâ–        | 14/100 [1:06:01<5:32:30, 231.98s/it]2025-08-31 13:42:00,457 | INFO | Computing rewards for 2 completions at step 7
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.263157894736842e-06, 'num_tokens': 35071.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.27000001072883606, 'rewards/reward_function/std': 0.0, 'reward': -0.27000001072883606, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.1540472507476807, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.3}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.157894736842105e-06, 'entropy': 1.2151904106140137, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.4}
2025-08-31 13:42:00,459 | INFO | Completion 0: format=-0.170, size=0.000, pixel=0.000, final_penalty=0.120, total=-0.290
2025-08-31 13:42:00,459 | INFO | Completion 1: format=-0.170, size=0.000, pixel=0.000, final_penalty=0.120, total=-0.290
2025-08-31 13:42:00,459 | INFO | Step 7 reward summary: avg=-0.2900Â±0.0000, range=[-0.290, -0.290], grids_found=0/2, perfect=0
2025-08-31 13:42:00,464 | INFO | Saved step 7 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00007.json
 16%|â–ˆâ–Œ        | 16/100 [1:15:17<5:22:59, 230.71s/it]2025-08-31 13:51:22,390 | INFO | Computing rewards for 2 completions at step 8
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.05263157894737e-06, 'num_tokens': 40087.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.28999999165534973, 'rewards/reward_function/std': 0.0, 'reward': -0.28999999165534973, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.7006981372833252, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.5}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.947368421052632e-06, 'entropy': 0.6976097822189331, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.6}
2025-08-31 13:51:22,391 | INFO | Completion 0: format=-0.180, size=0.000, pixel=0.000, final_penalty=0.130, total=-0.310
2025-08-31 13:51:22,391 | INFO | Completion 1: format=-0.180, size=0.000, pixel=0.000, final_penalty=0.130, total=-0.310
2025-08-31 13:51:22,392 | INFO | Step 8 reward summary: avg=-0.3100Â±0.0000, range=[-0.310, -0.310], grids_found=0/2, perfect=0
2025-08-31 13:51:22,408 | INFO | Saved step 8 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00008.json
 18%|â–ˆâ–Š        | 18/100 [1:24:39<5:16:20, 231.47s/it]2025-08-31 14:00:41,440 | INFO | Computing rewards for 2 completions at step 9
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.842105263157895e-06, 'num_tokens': 44541.0, 'completions/mean_length': 1715.0, 'completions/min_length': 1382.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 1382.0, 'completions/min_terminated_length': 1382.0, 'completions/max_terminated_length': 1382.0, 'rewards/reward_function/mean': -0.3100000023841858, 'rewards/reward_function/std': 0.0, 'reward': -0.3100000023841858, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.5334177613258362, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.7}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.736842105263158e-06, 'entropy': 0.5038031935691833, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.8}
2025-08-31 14:00:41,441 | INFO | Completion 0: format=-0.190, size=0.000, pixel=0.000, final_penalty=0.140, total=-0.330
2025-08-31 14:00:41,441 | INFO | Completion 1: format=-0.190, size=0.000, pixel=0.000, final_penalty=0.140, total=-0.330
2025-08-31 14:00:41,442 | INFO | Step 9 reward summary: avg=-0.3300Â±0.0000, range=[-0.330, -0.330], grids_found=0/2, perfect=0
2025-08-31 14:00:41,464 | INFO | Saved step 9 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00009.json
 20%|â–ˆâ–ˆ        | 20/100 [1:33:57<5:08:14, 231.18s/it]/data/miniforge3/envs/gptoss/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.631578947368422e-06, 'num_tokens': 49661.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.33000001311302185, 'rewards/reward_function/std': 0.0, 'reward': -0.33000001311302185, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.8966317176818848, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.9}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.526315789473685e-06, 'entropy': 0.8612761497497559, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.0}
  warnings.warn(
2025-08-31 14:09:58,201 | INFO | Computing rewards for 2 completions at step 10
2025-08-31 14:09:58,202 | INFO | Completion 0: format=-0.200, size=0.000, pixel=0.000, final_penalty=0.150, total=-0.350
2025-08-31 14:09:58,203 | INFO | Completion 1: format=-0.200, size=0.000, pixel=0.000, final_penalty=0.150, total=-0.350
2025-08-31 14:09:58,203 | INFO | Step 10 reward summary: avg=-0.3500Â±0.0000, range=[-0.350, -0.350], grids_found=0/2, perfect=0
2025-08-31 14:09:58,209 | INFO | Saved step 10 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00010.json
 22%|â–ˆâ–ˆâ–       | 22/100 [1:43:14<4:59:48, 230.62s/it]2025-08-31 14:19:17,359 | INFO | Computing rewards for 2 completions at step 11
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.421052631578948e-06, 'num_tokens': 54781.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.3499999940395355, 'rewards/reward_function/std': 0.0, 'reward': -0.3499999940395355, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.8498367071151733, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.1}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.315789473684212e-06, 'entropy': 1.0801150798797607, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.2}
2025-08-31 14:19:17,387 | INFO | Completion 0: format=-0.210, size=0.000, pixel=0.000, final_penalty=0.160, total=-0.370
2025-08-31 14:19:17,388 | INFO | Completion 1: format=-0.210, size=0.000, pixel=0.000, final_penalty=0.160, total=-0.370
2025-08-31 14:19:17,388 | INFO | Step 11 reward summary: avg=-0.3700Â±0.0000, range=[-0.370, -0.370], grids_found=0/2, perfect=0
2025-08-31 14:19:17,432 | INFO | Saved step 11 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00011.json
 24%|â–ˆâ–ˆâ–       | 24/100 [1:52:33<4:52:24, 230.84s/it]2025-08-31 14:28:33,458 | INFO | Computing rewards for 2 completions at step 12
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.210526315789475e-06, 'num_tokens': 59480.0, 'completions/mean_length': 1837.5, 'completions/min_length': 1627.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 1627.0, 'completions/min_terminated_length': 1627.0, 'completions/max_terminated_length': 1627.0, 'rewards/reward_function/mean': -0.3700000047683716, 'rewards/reward_function/std': 0.0, 'reward': -0.3700000047683716, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.5017120838165283, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.3}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.105263157894736e-06, 'entropy': 0.5553252696990967, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.4}
2025-08-31 14:28:33,459 | INFO | Completion 0: format=-0.220, size=0.000, pixel=0.000, final_penalty=0.170, total=-0.390
2025-08-31 14:28:33,460 | INFO | Completion 1: format=-0.220, size=0.000, pixel=0.000, final_penalty=0.170, total=-0.390
2025-08-31 14:28:33,460 | INFO | Step 12 reward summary: avg=-0.3900Â±0.0000, range=[-0.390, -0.390], grids_found=0/2, perfect=0
2025-08-31 14:28:33,465 | INFO | Saved step 12 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00012.json
 26%|â–ˆâ–ˆâ–Œ       | 26/100 [2:01:49<4:44:07, 230.37s/it]2025-08-31 14:37:54,299 | INFO | Computing rewards for 2 completions at step 13
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.000000000000001e-06, 'num_tokens': 64496.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.38999998569488525, 'rewards/reward_function/std': 0.0, 'reward': -0.38999998569488525, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.7797176837921143, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.5}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.894736842105265e-06, 'entropy': 0.5811101198196411, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.6}
2025-08-31 14:37:54,301 | INFO | Completion 0: format=-0.230, size=0.000, pixel=0.000, final_penalty=0.180, total=-0.410
2025-08-31 14:37:54,301 | INFO | Completion 1: format=-0.230, size=0.000, pixel=0.000, final_penalty=0.180, total=-0.410
2025-08-31 14:37:54,301 | INFO | Step 13 reward summary: avg=-0.4100Â±0.0000, range=[-0.410, -0.410], grids_found=0/2, perfect=0
2025-08-31 14:37:54,306 | INFO | Saved step 13 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00013.json
 28%|â–ˆâ–ˆâ–Š       | 28/100 [2:11:10<4:37:13, 231.02s/it]2025-08-31 14:47:18,321 | INFO | Computing rewards for 2 completions at step 14
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.789473684210526e-06, 'num_tokens': 69616.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.4099999964237213, 'rewards/reward_function/std': 0.0, 'reward': -0.4099999964237213, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.0771580934524536, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.7}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.68421052631579e-06, 'entropy': 0.9369204044342041, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.8}
2025-08-31 14:47:18,322 | INFO | Completion 0: format=-0.240, size=0.000, pixel=0.000, final_penalty=0.190, total=-0.430
2025-08-31 14:47:18,322 | INFO | Completion 1: format=-0.240, size=0.000, pixel=0.000, final_penalty=0.190, total=-0.430
2025-08-31 14:47:18,323 | INFO | Step 14 reward summary: avg=-0.4300Â±0.0000, range=[-0.430, -0.430], grids_found=0/2, perfect=0
2025-08-31 14:47:18,328 | INFO | Saved step 14 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00014.json
 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [2:20:34<4:30:46, 232.10s/it]/data/miniforge3/envs/gptoss/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.578947368421054e-06, 'num_tokens': 74736.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.4300000071525574, 'rewards/reward_function/std': 0.0, 'reward': -0.4300000071525574, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9059935808181763, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.9}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.473684210526316e-06, 'entropy': 1.0869845151901245, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.0}
  warnings.warn(
2025-08-31 14:56:42,088 | INFO | Computing rewards for 2 completions at step 15
2025-08-31 14:56:42,089 | INFO | Completion 0: format=-0.250, size=0.000, pixel=0.000, final_penalty=0.200, total=-0.450
2025-08-31 14:56:42,089 | INFO | Completion 1: format=-0.250, size=0.000, pixel=0.000, final_penalty=0.200, total=-0.450
2025-08-31 14:56:42,089 | INFO | Step 15 reward summary: avg=-0.4500Â±0.0000, range=[-0.450, -0.450], grids_found=0/2, perfect=0
2025-08-31 14:56:42,111 | INFO | Saved step 15 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00015.json
 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [2:29:58<4:23:33, 232.56s/it]2025-08-31 15:06:00,465 | INFO | Computing rewards for 2 completions at step 16
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.368421052631579e-06, 'num_tokens': 79856.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.44999998807907104, 'rewards/reward_function/std': 0.0, 'reward': -0.44999998807907104, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.912163257598877, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.1}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.263157894736843e-06, 'entropy': 0.6182047128677368, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.2}
2025-08-31 15:06:00,466 | INFO | Completion 0: format=-0.260, size=0.000, pixel=0.000, final_penalty=0.210, total=-0.470
2025-08-31 15:06:00,466 | INFO | Completion 1: format=-0.260, size=0.000, pixel=0.000, final_penalty=0.210, total=-0.470
2025-08-31 15:06:00,467 | INFO | Step 16 reward summary: avg=-0.4700Â±0.0000, range=[-0.470, -0.470], grids_found=0/2, perfect=0
2025-08-31 15:06:00,491 | INFO | Saved step 16 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00016.json
 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [2:39:17<4:14:52, 231.71s/it]2025-08-31 15:15:16,564 | INFO | Computing rewards for 2 completions at step 17
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.157894736842106e-06, 'num_tokens': 84872.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.4699999988079071, 'rewards/reward_function/std': 0.0, 'reward': -0.4699999988079071, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.7437050342559814, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.3}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.052631578947369e-06, 'entropy': 0.7083057165145874, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.4}
2025-08-31 15:15:16,565 | INFO | Completion 0: format=-0.270, size=0.000, pixel=0.000, final_penalty=0.220, total=-0.490
2025-08-31 15:15:16,565 | INFO | Completion 1: format=-0.270, size=0.000, pixel=0.000, final_penalty=0.220, total=-0.490
2025-08-31 15:15:16,566 | INFO | Step 17 reward summary: avg=-0.4900Â±0.0000, range=[-0.490, -0.490], grids_found=0/2, perfect=0
2025-08-31 15:15:16,589 | INFO | Saved step 17 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00017.json
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [2:48:33<4:06:06, 230.72s/it]2025-08-31 15:24:38,999 | INFO | Computing rewards for 2 completions at step 18
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.947368421052632e-06, 'num_tokens': 89992.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.49000000953674316, 'rewards/reward_function/std': 0.0, 'reward': -0.49000000953674316, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.7100473642349243, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.5}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.842105263157896e-06, 'entropy': 0.9185389280319214, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.6}
2025-08-31 15:24:39,000 | INFO | Completion 0: format=-0.280, size=0.000, pixel=0.000, final_penalty=0.230, total=-0.510
2025-08-31 15:24:39,000 | INFO | Completion 1: format=-0.280, size=0.000, pixel=0.000, final_penalty=0.230, total=-0.510
2025-08-31 15:24:39,000 | INFO | Step 18 reward summary: avg=-0.5100Â±0.0000, range=[-0.510, -0.510], grids_found=0/2, perfect=0
2025-08-31 15:24:39,060 | INFO | Saved step 18 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00018.json
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [2:57:55<3:59:18, 231.59s/it]2025-08-31 15:33:51,088 | INFO | Computing rewards for 2 completions at step 19
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.736842105263158e-06, 'num_tokens': 95112.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.5099999904632568, 'rewards/reward_function/std': 0.0, 'reward': -0.5099999904632568, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.1949381828308105, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.7}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.631578947368421e-06, 'entropy': 0.7993044257164001, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.8}
2025-08-31 15:33:51,089 | INFO | Completion 0: format=-0.290, size=0.000, pixel=0.000, final_penalty=0.240, total=-0.530
2025-08-31 15:33:51,089 | INFO | Completion 1: format=-0.290, size=0.000, pixel=0.000, final_penalty=0.240, total=-0.530
2025-08-31 15:33:51,090 | INFO | Step 19 reward summary: avg=-0.5300Â±0.0000, range=[-0.530, -0.530], grids_found=0/2, perfect=0
2025-08-31 15:33:51,111 | INFO | Saved step 19 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00019.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [3:07:07<3:49:48, 229.81s/it]/data/miniforge3/envs/gptoss/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.526315789473685e-06, 'num_tokens': 100232.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.5299999713897705, 'rewards/reward_function/std': 0.0, 'reward': -0.5299999713897705, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9100158214569092, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.9}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.421052631578948e-06, 'entropy': 0.8598930835723877, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.0}
  warnings.warn(
2025-08-31 15:43:06,124 | INFO | Computing rewards for 2 completions at step 20
2025-08-31 15:43:06,125 | INFO | Completion 0: format=-0.300, size=0.000, pixel=0.000, final_penalty=0.250, total=-0.550
2025-08-31 15:43:06,126 | INFO | Completion 1: format=-0.300, size=0.000, pixel=0.000, final_penalty=0.250, total=-0.550
2025-08-31 15:43:06,126 | INFO | Step 20 reward summary: avg=-0.5500Â±0.0000, range=[-0.550, -0.550], grids_found=0/2, perfect=0
2025-08-31 15:43:06,132 | INFO | Saved step 20 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00020.json
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [3:16:22<3:41:56, 229.60s/it]2025-08-31 15:52:26,905 | INFO | Computing rewards for 2 completions at step 21
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.31578947368421e-06, 'num_tokens': 105352.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.550000011920929, 'rewards/reward_function/std': 0.0, 'reward': -0.550000011920929, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.8317708969116211, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.1}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.2105263157894745e-06, 'entropy': 1.0028647184371948, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.2}
2025-08-31 15:52:26,906 | INFO | Completion 0: format=-0.310, size=0.000, pixel=0.000, final_penalty=0.260, total=-0.570
2025-08-31 15:52:26,906 | INFO | Completion 1: format=-0.310, size=0.000, pixel=0.000, final_penalty=0.260, total=-0.570
2025-08-31 15:52:26,907 | INFO | Step 21 reward summary: avg=-0.5700Â±0.0000, range=[-0.570, -0.570], grids_found=0/2, perfect=0
2025-08-31 15:52:26,911 | INFO | Saved step 21 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00021.json
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [3:25:43<3:35:18, 230.70s/it]2025-08-31 16:01:39,527 | INFO | Computing rewards for 2 completions at step 22
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.105263157894738e-06, 'num_tokens': 110472.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.5699999928474426, 'rewards/reward_function/std': 0.0, 'reward': -0.5699999928474426, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.995742678642273, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.3}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6e-06, 'entropy': 0.657320499420166, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.4}
2025-08-31 16:01:39,528 | INFO | Completion 0: format=-0.320, size=0.000, pixel=0.000, final_penalty=0.270, total=-0.590
2025-08-31 16:01:39,528 | INFO | Completion 1: format=-0.320, size=0.000, pixel=0.000, final_penalty=0.270, total=-0.590
2025-08-31 16:01:39,528 | INFO | Step 22 reward summary: avg=-0.5900Â±0.0000, range=[-0.590, -0.590], grids_found=0/2, perfect=0
2025-08-31 16:01:39,533 | INFO | Saved step 22 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00022.json
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [3:34:55<3:26:31, 229.48s/it]2025-08-31 16:10:51,875 | INFO | Computing rewards for 2 completions at step 23
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.8947368421052634e-06, 'num_tokens': 115592.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.5899999737739563, 'rewards/reward_function/std': 0.0, 'reward': -0.5899999737739563, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.8460954427719116, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.5}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.789473684210527e-06, 'entropy': 0.5729696154594421, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.6}
2025-08-31 16:10:51,876 | INFO | Completion 0: format=-0.330, size=0.000, pixel=0.000, final_penalty=0.280, total=-0.610
2025-08-31 16:10:51,876 | INFO | Completion 1: format=-0.330, size=0.000, pixel=0.000, final_penalty=0.280, total=-0.610
2025-08-31 16:10:51,877 | INFO | Step 23 reward summary: avg=-0.6100Â±0.0000, range=[-0.610, -0.610], grids_found=0/2, perfect=0
2025-08-31 16:10:51,907 | INFO | Saved step 23 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00023.json
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [3:44:08<3:18:25, 228.94s/it]2025-08-31 16:20:28,846 | INFO | Computing rewards for 2 completions at step 24
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.68421052631579e-06, 'num_tokens': 120608.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.6100000143051147, 'rewards/reward_function/std': 0.0, 'reward': -0.6100000143051147, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.6775892972946167, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.7}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.578947368421052e-06, 'entropy': 0.6038945317268372, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.8}
2025-08-31 16:20:28,874 | INFO | Completion 0: format=-0.340, size=0.000, pixel=0.000, final_penalty=0.290, total=-0.630
2025-08-31 16:20:28,875 | INFO | Completion 1: format=-0.340, size=0.000, pixel=0.000, final_penalty=0.290, total=-0.630
2025-08-31 16:20:28,875 | INFO | Step 24 reward summary: avg=-0.6300Â±0.0000, range=[-0.630, -0.630], grids_found=0/2, perfect=0
2025-08-31 16:20:28,884 | INFO | Saved step 24 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00024.json
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [3:53:45<3:14:49, 233.80s/it]/data/miniforge3/envs/gptoss/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.4736842105263165e-06, 'num_tokens': 125728.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.6299999952316284, 'rewards/reward_function/std': 0.0, 'reward': -0.6299999952316284, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.291135311126709, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.9}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.36842105263158e-06, 'entropy': 0.428812175989151, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.0}
  warnings.warn(
2025-08-31 16:29:46,809 | INFO | Computing rewards for 2 completions at step 25
2025-08-31 16:29:46,810 | INFO | Completion 0: format=-0.350, size=0.000, pixel=0.000, final_penalty=0.300, total=-0.650
2025-08-31 16:29:46,811 | INFO | Completion 1: format=-0.350, size=0.000, pixel=0.000, final_penalty=0.300, total=-0.650
2025-08-31 16:29:46,811 | INFO | Step 25 reward summary: avg=-0.6500Â±0.0000, range=[-0.650, -0.650], grids_found=0/2, perfect=0
2025-08-31 16:29:46,832 | INFO | Saved step 25 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00025.json
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [4:03:03<3:05:39, 232.07s/it]2025-08-31 16:39:06,490 | INFO | Computing rewards for 2 completions at step 26
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.263157894736842e-06, 'num_tokens': 130848.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.6499999761581421, 'rewards/reward_function/std': 0.0, 'reward': -0.6499999761581421, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9391036033630371, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.1}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.157894736842106e-06, 'entropy': 1.1397340297698975, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.2}
2025-08-31 16:39:06,491 | INFO | Completion 0: format=-0.360, size=0.000, pixel=0.000, final_penalty=0.310, total=-0.670
2025-08-31 16:39:06,491 | INFO | Completion 1: format=-0.360, size=0.000, pixel=0.000, final_penalty=0.310, total=-0.670
2025-08-31 16:39:06,491 | INFO | Step 26 reward summary: avg=-0.6700Â±0.0000, range=[-0.670, -0.670], grids_found=0/2, perfect=0
2025-08-31 16:39:06,496 | INFO | Saved step 26 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00026.json
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [4:12:22<2:57:40, 231.74s/it]2025-08-31 16:48:29,034 | INFO | Computing rewards for 2 completions at step 27
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.052631578947369e-06, 'num_tokens': 135968.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.6700000166893005, 'rewards/reward_function/std': 0.0, 'reward': -0.6700000166893005, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.8007336854934692, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.3}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.947368421052632e-06, 'entropy': 0.9952661991119385, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.4}
2025-08-31 16:48:29,035 | INFO | Completion 0: format=-0.370, size=0.000, pixel=0.000, final_penalty=0.320, total=-0.690
2025-08-31 16:48:29,036 | INFO | Completion 1: format=-0.370, size=0.000, pixel=0.000, final_penalty=0.320, total=-0.690
2025-08-31 16:48:29,036 | INFO | Step 27 reward summary: avg=-0.6900Â±0.0000, range=[-0.690, -0.690], grids_found=0/2, perfect=0
2025-08-31 16:48:29,041 | INFO | Saved step 27 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00027.json
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [4:21:45<2:50:16, 232.18s/it]2025-08-31 16:57:49,734 | INFO | Computing rewards for 2 completions at step 28
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.842105263157895e-06, 'num_tokens': 140984.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.6899999976158142, 'rewards/reward_function/std': 0.0, 'reward': -0.6899999976158142, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.8149476051330566, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.5}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.736842105263158e-06, 'entropy': 0.841954231262207, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.6}
2025-08-31 16:57:49,734 | INFO | Completion 0: format=-0.380, size=0.000, pixel=0.000, final_penalty=0.330, total=-0.710
2025-08-31 16:57:49,735 | INFO | Completion 1: format=-0.380, size=0.000, pixel=0.000, final_penalty=0.330, total=-0.710
2025-08-31 16:57:49,735 | INFO | Step 28 reward summary: avg=-0.7100Â±0.0000, range=[-0.710, -0.710], grids_found=0/2, perfect=0
2025-08-31 16:57:49,740 | INFO | Saved step 28 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00028.json
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [4:31:06<2:42:17, 231.84s/it]2025-08-31 17:07:10,806 | INFO | Computing rewards for 2 completions at step 29
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.631578947368421e-06, 'num_tokens': 146104.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.7099999785423279, 'rewards/reward_function/std': 0.0, 'reward': -0.7099999785423279, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9937161207199097, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.7}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.526315789473685e-06, 'entropy': 0.8841307163238525, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.8}
2025-08-31 17:07:10,815 | INFO | Completion 0: format=-0.390, size=0.000, pixel=0.000, final_penalty=0.340, total=-0.730
2025-08-31 17:07:10,815 | INFO | Completion 1: format=-0.390, size=0.000, pixel=0.000, final_penalty=0.340, total=-0.730
2025-08-31 17:07:10,816 | INFO | Step 29 reward summary: avg=-0.7300Â±0.0000, range=[-0.730, -0.730], grids_found=0/2, perfect=0
2025-08-31 17:07:10,822 | INFO | Saved step 29 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00029.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [4:40:27<2:34:35, 231.89s/it]/data/miniforge3/envs/gptoss/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.4210526315789476e-06, 'num_tokens': 151224.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.7300000190734863, 'rewards/reward_function/std': 0.0, 'reward': -0.7300000190734863, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9754046201705933, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.9}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.315789473684211e-06, 'entropy': 0.6968480348587036, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 6.0}
  warnings.warn(
2025-08-31 17:16:42,287 | INFO | Computing rewards for 2 completions at step 30
2025-08-31 17:16:42,288 | INFO | Completion 0: format=-0.400, size=0.000, pixel=0.000, final_penalty=0.350, total=-0.750
2025-08-31 17:16:42,288 | INFO | Completion 1: format=-0.400, size=0.000, pixel=0.000, final_penalty=0.350, total=-0.750
2025-08-31 17:16:42,289 | INFO | Step 30 reward summary: avg=-0.7500Â±0.0000, range=[-0.750, -0.750], grids_found=0/2, perfect=0
2025-08-31 17:16:42,294 | INFO | Saved step 30 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00030.json
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [4:49:58<2:28:12, 234.01s/it]2025-08-31 17:26:05,561 | INFO | Computing rewards for 2 completions at step 31
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.210526315789474e-06, 'num_tokens': 156344.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.75, 'rewards/reward_function/std': 0.0, 'reward': -0.75, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.4019612073898315, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 6.1}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.105263157894737e-06, 'entropy': 0.6965484023094177, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 6.2}
2025-08-31 17:26:05,562 | INFO | Completion 0: format=-0.410, size=0.000, pixel=0.000, final_penalty=0.360, total=-0.770
2025-08-31 17:26:05,562 | INFO | Completion 1: format=-0.410, size=0.000, pixel=0.000, final_penalty=0.360, total=-0.770
2025-08-31 17:26:05,563 | INFO | Step 31 reward summary: avg=-0.7700Â±0.0000, range=[-0.770, -0.770], grids_found=0/2, perfect=0
2025-08-31 17:26:05,568 | INFO | Saved step 31 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00031.json
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [4:59:22<2:20:03, 233.44s/it]2025-08-31 17:35:24,217 | INFO | Computing rewards for 2 completions at step 32
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.000000000000001e-06, 'num_tokens': 161079.0, 'completions/mean_length': 1855.5, 'completions/min_length': 1663.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 1663.0, 'completions/min_terminated_length': 1663.0, 'completions/max_terminated_length': 1663.0, 'rewards/reward_function/mean': -0.7699999809265137, 'rewards/reward_function/std': 0.0, 'reward': -0.7699999809265137, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.8382614850997925, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 6.3}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.894736842105263e-06, 'entropy': 0.5758306384086609, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 6.4}
2025-08-31 17:35:24,218 | INFO | Completion 0: format=-0.420, size=0.000, pixel=0.000, final_penalty=0.370, total=-0.790
2025-08-31 17:35:24,218 | INFO | Completion 1: format=-0.420, size=0.000, pixel=0.000, final_penalty=0.370, total=-0.790
2025-08-31 17:35:24,219 | INFO | Step 32 reward summary: avg=-0.7900Â±0.0000, range=[-0.790, -0.790], grids_found=0/2, perfect=0
2025-08-31 17:35:24,236 | INFO | Saved step 32 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00032.json
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [5:08:40<2:11:31, 232.09s/it]2025-08-31 17:44:49,261 | INFO | Computing rewards for 2 completions at step 33
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.789473684210527e-06, 'num_tokens': 166199.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.7900000214576721, 'rewards/reward_function/std': 0.0, 'reward': -0.7900000214576721, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9652025699615479, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 6.5}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.6842105263157896e-06, 'entropy': 0.9730568528175354, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 6.6}
2025-08-31 17:44:49,262 | INFO | Completion 0: format=-0.430, size=0.000, pixel=0.000, final_penalty=0.380, total=-0.810
2025-08-31 17:44:49,262 | INFO | Completion 1: format=-0.430, size=0.000, pixel=0.000, final_penalty=0.380, total=-0.810
2025-08-31 17:44:49,263 | INFO | Step 33 reward summary: avg=-0.8100Â±0.0000, range=[-0.810, -0.810], grids_found=0/2, perfect=0
2025-08-31 17:44:49,269 | INFO | Saved step 33 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00033.json
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [5:18:05<2:04:11, 232.85s/it]2025-08-31 17:54:12,132 | INFO | Computing rewards for 2 completions at step 34
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.578947368421053e-06, 'num_tokens': 171319.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.8100000023841858, 'rewards/reward_function/std': 0.0, 'reward': -0.8100000023841858, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9933115839958191, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 6.7}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.473684210526316e-06, 'entropy': 0.7648164629936218, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 6.8}
2025-08-31 17:54:12,133 | INFO | Completion 0: format=-0.440, size=0.000, pixel=0.000, final_penalty=0.390, total=-0.830
2025-08-31 17:54:12,133 | INFO | Completion 1: format=-0.440, size=0.000, pixel=0.000, final_penalty=0.390, total=-0.830
2025-08-31 17:54:12,134 | INFO | Step 34 reward summary: avg=-0.8300Â±0.0000, range=[-0.830, -0.830], grids_found=0/2, perfect=0
2025-08-31 17:54:12,158 | INFO | Saved step 34 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00034.json
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [5:27:28<1:56:23, 232.79s/it]/data/miniforge3/envs/gptoss/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.368421052631579e-06, 'num_tokens': 176335.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.8299999833106995, 'rewards/reward_function/std': 0.0, 'reward': -0.8299999833106995, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.7115254402160645, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 6.9}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.2631578947368423e-06, 'entropy': 0.7473138570785522, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 7.0}
  warnings.warn(
2025-08-31 18:03:36,453 | INFO | Computing rewards for 2 completions at step 35
2025-08-31 18:03:36,454 | INFO | Completion 0: format=-0.450, size=0.000, pixel=0.000, final_penalty=0.400, total=-0.850
2025-08-31 18:03:36,454 | INFO | Completion 1: format=-0.450, size=0.000, pixel=0.000, final_penalty=0.400, total=-0.850
2025-08-31 18:03:36,455 | INFO | Step 35 reward summary: avg=-0.8500Â±0.0000, range=[-0.850, -0.850], grids_found=0/2, perfect=0
2025-08-31 18:03:36,460 | INFO | Saved step 35 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00035.json
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [5:36:52<1:48:41, 232.92s/it]2025-08-31 18:12:58,817 | INFO | Computing rewards for 2 completions at step 36
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.157894736842105e-06, 'num_tokens': 181455.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.8500000238418579, 'rewards/reward_function/std': 0.0, 'reward': -0.8500000238418579, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.5128942728042603, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 7.1}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.052631578947369e-06, 'entropy': 1.035086989402771, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 7.2}
2025-08-31 18:12:58,818 | INFO | Completion 0: format=-0.460, size=0.000, pixel=0.000, final_penalty=0.410, total=-0.870
2025-08-31 18:12:58,819 | INFO | Completion 1: format=-0.460, size=0.000, pixel=0.000, final_penalty=0.410, total=-0.870
2025-08-31 18:12:58,819 | INFO | Step 36 reward summary: avg=-0.8700Â±0.0000, range=[-0.870, -0.870], grids_found=0/2, perfect=0
2025-08-31 18:12:58,824 | INFO | Saved step 36 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00036.json
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [5:46:14<1:40:47, 232.59s/it]2025-08-31 18:22:10,099 | INFO | Computing rewards for 2 completions at step 37
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.9473684210526317e-06, 'num_tokens': 186575.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.8700000047683716, 'rewards/reward_function/std': 0.0, 'reward': -0.8700000047683716, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.2591609954833984, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 7.3}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.842105263157895e-06, 'entropy': 1.070202350616455, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 7.4}
2025-08-31 18:22:10,100 | INFO | Completion 0: format=-0.470, size=0.000, pixel=0.000, final_penalty=0.420, total=-0.890
2025-08-31 18:22:10,101 | INFO | Completion 1: format=-0.470, size=0.000, pixel=0.000, final_penalty=0.420, total=-0.890
2025-08-31 18:22:10,101 | INFO | Step 37 reward summary: avg=-0.8900Â±0.0000, range=[-0.890, -0.890], grids_found=0/2, perfect=0
2025-08-31 18:22:10,105 | INFO | Saved step 37 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00037.json
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [5:55:26<1:32:03, 230.17s/it]2025-08-31 18:31:23,065 | INFO | Computing rewards for 2 completions at step 38
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.7368421052631583e-06, 'num_tokens': 191695.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.8899999856948853, 'rewards/reward_function/std': 0.0, 'reward': -0.8899999856948853, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.7922742366790771, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 7.5}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.631578947368421e-06, 'entropy': 0.8128307461738586, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 7.6}
2025-08-31 18:31:23,066 | INFO | Completion 0: format=-0.480, size=0.000, pixel=0.000, final_penalty=0.430, total=-0.910
2025-08-31 18:31:23,067 | INFO | Completion 1: format=-0.480, size=0.000, pixel=0.000, final_penalty=0.430, total=-0.910
2025-08-31 18:31:23,067 | INFO | Step 38 reward summary: avg=-0.9100Â±0.0000, range=[-0.910, -0.910], grids_found=0/2, perfect=0
2025-08-31 18:31:23,071 | INFO | Saved step 38 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00038.json
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [6:04:39<1:24:08, 229.47s/it]2025-08-31 18:41:08,535 | INFO | Computing rewards for 2 completions at step 39
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.5263157894736844e-06, 'num_tokens': 196711.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.9100000262260437, 'rewards/reward_function/std': 0.0, 'reward': -0.9100000262260437, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.84685879945755, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 7.7}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.4210526315789477e-06, 'entropy': 0.7205436825752258, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 7.8}
2025-08-31 18:41:08,536 | INFO | Completion 0: format=-0.490, size=0.000, pixel=0.000, final_penalty=0.440, total=-0.930
2025-08-31 18:41:08,537 | INFO | Completion 1: format=-0.490, size=0.000, pixel=0.000, final_penalty=0.440, total=-0.930
2025-08-31 18:41:08,537 | INFO | Step 39 reward summary: avg=-0.9300Â±0.0000, range=[-0.930, -0.930], grids_found=0/2, perfect=0
2025-08-31 18:41:08,541 | INFO | Saved step 39 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00039.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [6:14:24<1:18:34, 235.72s/it]/data/miniforge3/envs/gptoss/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.3157894736842105e-06, 'num_tokens': 200339.0, 'completions/mean_length': 1302.0, 'completions/min_length': 556.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 556.0, 'completions/min_terminated_length': 556.0, 'completions/max_terminated_length': 556.0, 'rewards/reward_function/mean': -0.9300000071525574, 'rewards/reward_function/std': 0.0, 'reward': -0.9300000071525574, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.0089000463485718, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 7.9}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.2105263157894738e-06, 'entropy': 0.9966962933540344, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 8.0}
  warnings.warn(
2025-08-31 18:50:28,139 | INFO | Computing rewards for 2 completions at step 40
2025-08-31 18:50:28,140 | INFO | Completion 0: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.450, total=-0.950
2025-08-31 18:50:28,141 | INFO | Completion 1: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.450, total=-0.950
2025-08-31 18:50:28,141 | INFO | Step 40 reward summary: avg=-0.9500Â±0.0000, range=[-0.950, -0.950], grids_found=0/2, perfect=0
2025-08-31 18:50:28,145 | INFO | Saved step 40 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00040.json
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [6:23:44<1:10:02, 233.45s/it]2025-08-31 18:59:52,244 | INFO | Computing rewards for 2 completions at step 41
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.105263157894737e-06, 'num_tokens': 205459.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.949999988079071, 'rewards/reward_function/std': 0.0, 'reward': -0.949999988079071, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.8662218451499939, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 8.1}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.0000000000000003e-06, 'entropy': 0.7407183647155762, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 8.2}
2025-08-31 18:59:52,245 | INFO | Completion 0: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.460, total=-0.960
2025-08-31 18:59:52,245 | INFO | Completion 1: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.460, total=-0.960
2025-08-31 18:59:52,246 | INFO | Step 41 reward summary: avg=-0.9600Â±0.0000, range=[-0.960, -0.960], grids_found=0/2, perfect=0
2025-08-31 18:59:52,249 | INFO | Saved step 41 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00041.json
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [6:33:08<1:02:12, 233.26s/it]2025-08-31 19:09:10,548 | INFO | Computing rewards for 2 completions at step 42
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.8947368421052634e-06, 'num_tokens': 210210.0, 'completions/mean_length': 1863.5, 'completions/min_length': 1679.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 1679.0, 'completions/min_terminated_length': 1679.0, 'completions/max_terminated_length': 1679.0, 'rewards/reward_function/mean': -0.9599999785423279, 'rewards/reward_function/std': 0.0, 'reward': -0.9599999785423279, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9005565643310547, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 8.3}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.7894736842105265e-06, 'entropy': 0.5978978872299194, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 8.4}
2025-08-31 19:09:10,549 | INFO | Completion 0: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.470, total=-0.970
2025-08-31 19:09:10,549 | INFO | Completion 1: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.470, total=-0.970
2025-08-31 19:09:10,550 | INFO | Step 42 reward summary: avg=-0.9700Â±0.0000, range=[-0.970, -0.970], grids_found=0/2, perfect=0
2025-08-31 19:09:10,554 | INFO | Saved step 42 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00042.json
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [6:42:26<54:07, 231.97s/it]2025-08-31 19:18:32,836 | INFO | Computing rewards for 2 completions at step 43
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.6842105263157895e-06, 'num_tokens': 215330.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.9700000286102295, 'rewards/reward_function/std': 0.0, 'reward': -0.9700000286102295, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.223913550376892, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 8.5}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.5789473684210526e-06, 'entropy': 1.1585230827331543, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 8.6}
2025-08-31 19:18:32,863 | INFO | Completion 0: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.480, total=-0.980
2025-08-31 19:18:32,864 | INFO | Completion 1: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.480, total=-0.980
2025-08-31 19:18:32,864 | INFO | Step 43 reward summary: avg=-0.9800Â±0.0000, range=[-0.980, -0.980], grids_found=0/2, perfect=0
2025-08-31 19:18:32,868 | INFO | Saved step 43 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00043.json
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [6:51:49<46:26, 232.20s/it]2025-08-31 19:27:51,445 | INFO | Computing rewards for 2 completions at step 44
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.4736842105263159e-06, 'num_tokens': 220346.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.9800000190734863, 'rewards/reward_function/std': 0.0, 'reward': -0.9800000190734863, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.8219314813613892, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 8.7}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.3684210526315791e-06, 'entropy': 0.6471485495567322, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 8.8}
2025-08-31 19:27:51,446 | INFO | Completion 0: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.490, total=-0.990
2025-08-31 19:27:51,446 | INFO | Completion 1: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.490, total=-0.990
2025-08-31 19:27:51,447 | INFO | Step 44 reward summary: avg=-0.9900Â±0.0000, range=[-0.990, -0.990], grids_found=0/2, perfect=0
2025-08-31 19:27:51,451 | INFO | Saved step 44 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00044.json
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [7:01:07<38:34, 231.49s/it]/data/miniforge3/envs/gptoss/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.2631578947368422e-06, 'num_tokens': 225466.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.9900000095367432, 'rewards/reward_function/std': 0.0, 'reward': -0.9900000095367432, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.0161248445510864, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 8.9}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.1578947368421053e-06, 'entropy': 0.8018126487731934, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 9.0}
  warnings.warn(
2025-08-31 19:37:19,866 | INFO | Computing rewards for 2 completions at step 45
2025-08-31 19:37:19,867 | INFO | Completion 0: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.500, total=-1.000
2025-08-31 19:37:19,867 | INFO | Completion 1: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.500, total=-1.000
2025-08-31 19:37:19,868 | INFO | Step 45 reward summary: avg=-1.0000Â±0.0000, range=[-1.000, -1.000], grids_found=0/2, perfect=0
2025-08-31 19:37:19,872 | INFO | Saved step 45 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00045.json
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [7:10:36<31:06, 233.28s/it]2025-08-31 19:46:49,561 | INFO | Computing rewards for 2 completions at step 46
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.0526315789473685e-06, 'num_tokens': 230586.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -1.0, 'rewards/reward_function/std': 0.0, 'reward': -1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.6690709590911865, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 9.1}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.473684210526317e-07, 'entropy': 1.1809639930725098, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 9.2}
2025-08-31 19:46:49,562 | INFO | Completion 0: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.500, total=-1.000
2025-08-31 19:46:49,563 | INFO | Completion 1: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.500, total=-1.000
2025-08-31 19:46:49,563 | INFO | Step 46 reward summary: avg=-1.0000Â±0.0000, range=[-1.000, -1.000], grids_found=0/2, perfect=0
2025-08-31 19:46:49,567 | INFO | Saved step 46 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00046.json
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [7:20:05<23:26, 234.35s/it]2025-08-31 19:56:09,743 | INFO | Computing rewards for 2 completions at step 47
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.421052631578948e-07, 'num_tokens': 235706.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -1.0, 'rewards/reward_function/std': 0.0, 'reward': -1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 1.4405699968338013, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 9.3}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.368421052631579e-07, 'entropy': 0.3970978558063507, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 9.4}
2025-08-31 19:56:09,744 | INFO | Completion 0: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.500, total=-1.000
2025-08-31 19:56:09,745 | INFO | Completion 1: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.500, total=-1.000
2025-08-31 19:56:09,745 | INFO | Step 47 reward summary: avg=-1.0000Â±0.0000, range=[-1.000, -1.000], grids_found=0/2, perfect=0
2025-08-31 19:56:09,749 | INFO | Saved step 47 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00047.json
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [7:29:26<15:31, 232.93s/it]2025-08-31 20:05:33,614 | INFO | Computing rewards for 2 completions at step 48
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.315789473684211e-07, 'num_tokens': 240722.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -1.0, 'rewards/reward_function/std': 0.0, 'reward': -1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.5756268501281738, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 9.5}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5.263157894736843e-07, 'entropy': 0.7415385246276855, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 9.6}
2025-08-31 20:05:33,615 | INFO | Completion 0: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.500, total=-1.000
2025-08-31 20:05:33,615 | INFO | Completion 1: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.500, total=-1.000
2025-08-31 20:05:33,616 | INFO | Step 48 reward summary: avg=-1.0000Â±0.0000, range=[-1.000, -1.000], grids_found=0/2, perfect=0
2025-08-31 20:05:33,619 | INFO | Saved step 48 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00048.json
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [7:38:49<07:45, 232.95s/it]2025-08-31 20:14:51,759 | INFO | Computing rewards for 2 completions at step 49
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.210526315789474e-07, 'num_tokens': 245842.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -1.0, 'rewards/reward_function/std': 0.0, 'reward': -1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.7555252313613892, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 9.7}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.1578947368421055e-07, 'entropy': 0.9470642805099487, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 9.8}
2025-08-31 20:14:51,760 | INFO | Completion 0: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.500, total=-1.000
2025-08-31 20:14:51,761 | INFO | Completion 1: format=-0.500, size=0.000, pixel=0.000, final_penalty=0.500, total=-1.000
2025-08-31 20:14:51,761 | INFO | Step 49 reward summary: avg=-1.0000Â±0.0000, range=[-1.000, -1.000], grids_found=0/2, perfect=0
2025-08-31 20:14:51,766 | INFO | Saved step 49 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00049.json
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [7:48:09<00:00, 280.89s/it]
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.105263157894737e-07, 'num_tokens': 250962.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -1.0, 'rewards/reward_function/std': 0.0, 'reward': -1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.9425229430198669, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 9.9}
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.0526315789473685e-07, 'entropy': 0.9747464656829834, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 10.0}
{'train_runtime': 28089.3007, 'train_samples_per_second': 0.004, 'train_steps_per_second': 0.004, 'train_loss': 0.0, 'epoch': 10.0}
2025-08-31 20:15:02,475 | INFO | âœ… Training completed!
2025-08-31 20:15:03,652 | INFO | ðŸ’¾ Model saved to /data/gpt_oss_final/final_model_hf_trl_dapo
