2025-08-27 17:24:47,435 | INFO | Loading model and tokenizer...
Using MXFP4 quantized models requires a GPU, we will default to dequantizing the model to bf16
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.28s/it]
trainable params: 7,962,624 || all params: 20,922,719,808 || trainable%: 0.0381
2025-08-27 17:26:50,689 | INFO | âœ… Model and tokenizer loaded successfully
2025-08-27 17:26:50,690 | INFO | ðŸ“Š Loaded 3 ARC problems
2025-08-27 17:26:51,737 | INFO | ðŸ“Š Created dataset with 3 examples
[2025-08-27 17:26:55,766] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-08-27 17:26:56,643 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -c /data/tmp/tmpkefk4qn7/test.c -o /data/tmp/tmpkefk4qn7/test.o
2025-08-27 17:26:56,677 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat /data/tmp/tmpkefk4qn7/test.o -laio -o /data/tmp/tmpkefk4qn7/a.out
2025-08-27 17:26:57,557 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -c /data/tmp/tmppsc5gusy/test.c -o /data/tmp/tmppsc5gusy/test.o
2025-08-27 17:26:57,590 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat /data/tmp/tmppsc5gusy/test.o -L/usr/local/cuda-12.6 -L/usr/local/cuda-12.6/lib64 -lcufile -o /data/tmp/tmppsc5gusy/a.out
2025-08-27 17:26:57,693 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -c /data/tmp/tmpzdek3fmn/test.c -o /data/tmp/tmpzdek3fmn/test.o
2025-08-27 17:26:57,726 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat /data/tmp/tmpzdek3fmn/test.o -laio -o /data/tmp/tmpzdek3fmn/a.out
[2025-08-27 17:26:59,172] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
2025-08-27 17:26:59,255 | INFO | ðŸš€ Starting HuggingFace TRL DAPO training...
2025-08-27 17:26:59,256 | INFO | ðŸ“ Using corrected Harmony format - no examples in input prompts
2025-08-27 17:26:59,256 | INFO |    Loss type: bnpo
2025-08-27 17:26:59,256 | INFO |    Epsilon: 0.2
2025-08-27 17:26:59,256 | INFO |    Epsilon high: 0.28
2025-08-27 17:26:59,256 | INFO |    Repetition penalty: 1.0
2025-08-27 17:26:59,256 | INFO |    Reward components: 4 (format + size + pixel + final_channel)
2025-08-27 17:26:59,597 | WARNING | Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 4. Using DeepSpeed's value.
Installed CUDA version 12.6 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/ubuntu/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
/data/miniforge3/envs/gpt_oss_rl/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.521625280380249 seconds
Parameter Offload - Persistent parameters statistics: param_count = 241, numel = 728640
  0%|          | 0/100 [00:00<?, ?it/s]2025-08-27 17:58:06,730 | INFO | Computing rewards for 2 completions at step 0
2025-08-27 17:58:06,733 | INFO | Completion 0: format=-0.100, size=0.000, pixel=0.000, final_penalty=0.050, total=-0.150
2025-08-27 17:58:06,734 | INFO | Completion 1: format=-0.100, size=0.000, pixel=0.000, final_penalty=0.050, total=-0.150
2025-08-27 17:58:06,734 | INFO | Step 0 reward summary: avg=-0.1500Â±0.0000, range=[-0.150, -0.150], grids_found=0/2, perfect=0
2025-08-27 17:58:06,742 | INFO | Saved step 0 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00000.json
2025-08-27 18:20:48,115 | INFO | Computing rewards for 2 completions at step 1
2025-08-27 18:20:48,118 | INFO | Completion 0: format=-0.110, size=0.000, pixel=0.000, final_penalty=0.060, total=-0.170
2025-08-27 18:20:48,118 | INFO | Completion 1: format=-0.110, size=0.000, pixel=0.000, final_penalty=0.060, total=-0.170
2025-08-27 18:20:48,119 | INFO | Step 1 reward summary: avg=-0.1700Â±0.0000, range=[-0.170, -0.170], grids_found=0/2, perfect=0
2025-08-27 18:20:48,142 | INFO | Saved step 1 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00001.json
  1%|          | 1/100 [50:53<83:57:34, 3053.08s/it]2025-08-27 18:42:08,958 | INFO | Computing rewards for 2 completions at step 2
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'num_tokens': 10240.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.1600000038743019, 'rewards/reward_function/std': 0.0, 'reward': -0.1600000038743019, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.41552734375, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.67}
2025-08-27 18:42:08,960 | INFO | Completion 0: format=-0.120, size=0.000, pixel=0.000, final_penalty=0.070, total=-0.190
2025-08-27 18:42:08,960 | INFO | Completion 1: format=-0.120, size=0.000, pixel=0.000, final_penalty=0.070, total=-0.190
2025-08-27 18:42:08,961 | INFO | Step 2 reward summary: avg=-0.1900Â±0.0000, range=[-0.190, -0.190], grids_found=0/2, perfect=0
2025-08-27 18:42:08,980 | INFO | Saved step 2 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00002.json
  2%|â–         | 2/100 [1:12:16<54:46:16, 2012.01s/it]2025-08-27 19:04:54,415 | INFO | Computing rewards for 2 completions at step 3
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.0000000000000003e-06, 'num_tokens': 15360.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.1899999976158142, 'rewards/reward_function/std': 0.0, 'reward': -0.1899999976158142, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.4736328125, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.0}
2025-08-27 19:04:54,453 | INFO | Completion 0: format=-0.130, size=0.000, pixel=0.000, final_penalty=0.080, total=-0.210
2025-08-27 19:04:54,453 | INFO | Completion 1: format=-0.130, size=0.000, pixel=0.000, final_penalty=0.080, total=-0.210
2025-08-27 19:04:54,454 | INFO | Step 3 reward summary: avg=-0.2100Â±0.0000, range=[-0.210, -0.210], grids_found=0/2, perfect=0
2025-08-27 19:04:54,477 | INFO | Saved step 3 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00003.json
2025-08-27 19:26:57,158 | INFO | Computing rewards for 2 completions at step 4
2025-08-27 19:26:57,160 | INFO | Completion 0: format=-0.140, size=0.000, pixel=0.000, final_penalty=0.090, total=-0.230
2025-08-27 19:26:57,160 | INFO | Completion 1: format=-0.140, size=0.000, pixel=0.000, final_penalty=0.090, total=-0.230
2025-08-27 19:26:57,161 | INFO | Step 4 reward summary: avg=-0.2300Â±0.0000, range=[-0.230, -0.230], grids_found=0/2, perfect=0
2025-08-27 19:26:57,203 | INFO | Saved step 4 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00004.json
  3%|â–Ž         | 3/100 [1:57:01<62:29:52, 2319.51s/it]2025-08-27 19:48:37,428 | INFO | Computing rewards for 2 completions at step 5
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.000000000000001e-06, 'num_tokens': 25600.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.2199999988079071, 'rewards/reward_function/std': 0.0, 'reward': -0.2199999988079071, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.453857421875, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.67}
2025-08-27 19:48:37,431 | INFO | Completion 0: format=-0.150, size=0.000, pixel=0.000, final_penalty=0.100, total=-0.250
2025-08-27 19:48:37,431 | INFO | Completion 1: format=-0.150, size=0.000, pixel=0.000, final_penalty=0.100, total=-0.250
2025-08-27 19:48:37,432 | INFO | Step 5 reward summary: avg=-0.2500Â±0.0000, range=[-0.250, -0.250], grids_found=0/2, perfect=0
2025-08-27 19:48:37,455 | INFO | Saved step 5 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00005.json
  4%|â–         | 4/100 [2:18:40<51:06:40, 1916.68s/it]2025-08-27 20:11:29,640 | INFO | Computing rewards for 2 completions at step 6
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6e-06, 'num_tokens': 30720.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.25, 'rewards/reward_function/std': 0.0, 'reward': -0.25, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.53515625, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.0}
2025-08-27 20:11:29,642 | INFO | Completion 0: format=-0.160, size=0.000, pixel=0.000, final_penalty=0.110, total=-0.270
2025-08-27 20:11:29,643 | INFO | Completion 1: format=-0.160, size=0.000, pixel=0.000, final_penalty=0.110, total=-0.270
2025-08-27 20:11:29,643 | INFO | Step 6 reward summary: avg=-0.2700Â±0.0000, range=[-0.270, -0.270], grids_found=0/2, perfect=0
2025-08-27 20:11:29,648 | INFO | Saved step 6 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00006.json
2025-08-27 20:33:06,340 | INFO | Computing rewards for 2 completions at step 7
2025-08-27 20:33:06,366 | INFO | Completion 0: format=-0.170, size=0.000, pixel=0.000, final_penalty=0.120, total=-0.290
2025-08-27 20:33:06,367 | INFO | Completion 1: format=-0.170, size=0.000, pixel=0.000, final_penalty=0.120, total=-0.290
2025-08-27 20:33:06,367 | INFO | Step 7 reward summary: avg=-0.2900Â±0.0000, range=[-0.290, -0.290], grids_found=0/2, perfect=0
2025-08-27 20:33:06,390 | INFO | Saved step 7 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00007.json
  5%|â–Œ         | 5/100 [3:03:10<57:44:45, 2188.27s/it]2025-08-27 20:55:06,624 | INFO | Computing rewards for 2 completions at step 8
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.000000000000001e-06, 'num_tokens': 40960.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.2800000011920929, 'rewards/reward_function/std': 0.0, 'reward': -0.2800000011920929, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.3446044921875, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.67}
2025-08-27 20:55:06,625 | INFO | Completion 0: format=-0.180, size=0.000, pixel=0.000, final_penalty=0.130, total=-0.310
2025-08-27 20:55:06,626 | INFO | Completion 1: format=-0.180, size=0.000, pixel=0.000, final_penalty=0.130, total=-0.310
2025-08-27 20:55:06,626 | INFO | Step 8 reward summary: avg=-0.3100Â±0.0000, range=[-0.310, -0.310], grids_found=0/2, perfect=0
2025-08-27 20:55:06,645 | INFO | Saved step 8 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00008.json
  6%|â–Œ         | 6/100 [3:25:11<49:26:18, 1893.39s/it]2025-08-27 21:16:44,588 | INFO | Computing rewards for 2 completions at step 9
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1e-05, 'num_tokens': 46080.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.3100000023841858, 'rewards/reward_function/std': 0.0, 'reward': -0.3100000023841858, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.4521484375, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.0}
2025-08-27 21:16:44,589 | INFO | Completion 0: format=-0.190, size=0.000, pixel=0.000, final_penalty=0.140, total=-0.330
2025-08-27 21:16:44,590 | INFO | Completion 1: format=-0.190, size=0.000, pixel=0.000, final_penalty=0.140, total=-0.330
2025-08-27 21:16:44,591 | INFO | Step 9 reward summary: avg=-0.3300Â±0.0000, range=[-0.330, -0.330], grids_found=0/2, perfect=0
2025-08-27 21:16:44,611 | INFO | Saved step 9 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00009.json
2025-08-27 21:38:37,278 | INFO | Computing rewards for 2 completions at step 10
2025-08-27 21:38:37,279 | INFO | Completion 0: format=-0.200, size=0.000, pixel=0.000, final_penalty=0.150, total=-0.350
2025-08-27 21:38:37,280 | INFO | Completion 1: format=-0.200, size=0.000, pixel=0.000, final_penalty=0.150, total=-0.350
2025-08-27 21:38:37,281 | INFO | Step 10 reward summary: avg=-0.3500Â±0.0000, range=[-0.350, -0.350], grids_found=0/2, perfect=0
2025-08-27 21:38:37,288 | INFO | Saved step 10 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00010.json
  7%|â–‹         | 7/100 [4:08:40<54:57:20, 2127.32s/it]2025-08-27 22:00:56,657 | INFO | Computing rewards for 2 completions at step 11
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.894736842105264e-06, 'num_tokens': 56320.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.3400000035762787, 'rewards/reward_function/std': 0.0, 'reward': -0.3400000035762787, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.6298828125, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 3.67}
2025-08-27 22:00:56,658 | INFO | Completion 0: format=-0.210, size=0.000, pixel=0.000, final_penalty=0.160, total=-0.370
2025-08-27 22:00:56,660 | INFO | Completion 1: format=-0.210, size=0.000, pixel=0.000, final_penalty=0.160, total=-0.370
2025-08-27 22:00:56,661 | INFO | Step 11 reward summary: avg=-0.3700Â±0.0000, range=[-0.370, -0.370], grids_found=0/2, perfect=0
2025-08-27 22:00:56,679 | INFO | Saved step 11 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00011.json
  8%|â–Š         | 8/100 [4:31:01<47:58:02, 1876.99s/it]2025-08-27 22:22:48,319 | INFO | Computing rewards for 2 completions at step 12
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.789473684210527e-06, 'num_tokens': 61440.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.3700000047683716, 'rewards/reward_function/std': 0.0, 'reward': -0.3700000047683716, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.46435546875, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.0}
2025-08-27 22:22:48,342 | INFO | Completion 0: format=-0.220, size=0.000, pixel=0.000, final_penalty=0.170, total=-0.390
2025-08-27 22:22:48,343 | INFO | Completion 1: format=-0.220, size=0.000, pixel=0.000, final_penalty=0.170, total=-0.390
2025-08-27 22:22:48,343 | INFO | Step 12 reward summary: avg=-0.3900Â±0.0000, range=[-0.390, -0.390], grids_found=0/2, perfect=0
2025-08-27 22:22:48,350 | INFO | Saved step 12 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00012.json
2025-08-27 22:44:22,460 | INFO | Computing rewards for 2 completions at step 13
2025-08-27 22:44:22,492 | INFO | Completion 0: format=-0.230, size=0.000, pixel=0.000, final_penalty=0.180, total=-0.410
2025-08-27 22:44:22,493 | INFO | Completion 1: format=-0.230, size=0.000, pixel=0.000, final_penalty=0.180, total=-0.410
2025-08-27 22:44:22,493 | INFO | Step 13 reward summary: avg=-0.4100Â±0.0000, range=[-0.410, -0.410], grids_found=0/2, perfect=0
2025-08-27 22:44:22,500 | INFO | Saved step 13 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00013.json
  9%|â–‰         | 9/100 [5:14:31<53:14:16, 2106.11s/it]2025-08-27 23:06:55,893 | INFO | Computing rewards for 2 completions at step 14
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.68421052631579e-06, 'num_tokens': 71680.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.3999999910593033, 'rewards/reward_function/std': 0.0, 'reward': -0.3999999910593033, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.366943359375, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 4.67}
2025-08-27 23:06:55,894 | INFO | Completion 0: format=-0.240, size=0.000, pixel=0.000, final_penalty=0.190, total=-0.430
2025-08-27 23:06:55,895 | INFO | Completion 1: format=-0.240, size=0.000, pixel=0.000, final_penalty=0.190, total=-0.430
2025-08-27 23:06:55,895 | INFO | Step 14 reward summary: avg=-0.4300Â±0.0000, range=[-0.430, -0.430], grids_found=0/2, perfect=0
2025-08-27 23:06:55,911 | INFO | Saved step 14 info to /data/gpt_oss_final/logs/hf_trl_responses/step_00014.json
 10%|â–ˆ         | 10/100 [5:37:00<46:48:42, 1872.47s/it]
{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.578947368421054e-06, 'num_tokens': 76800.0, 'completions/mean_length': 2048.0, 'completions/min_length': 2048.0, 'completions/max_length': 2048.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_function/mean': -0.4300000071525574, 'rewards/reward_function/std': 0.0, 'reward': -0.4300000071525574, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.4794921875, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 5.0}
