2025-08-26 00:40:35,984 | INFO | Loading model and tokenizer...
Using MXFP4 quantized models requires a GPU, we will default to dequantizing the model to bf16
2025-08-26 00:40:52,007 | INFO | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:12<00:00, 24.04s/it]
trainable params: 7,962,624 || all params: 20,922,719,808 || trainable%: 0.0381
2025-08-26 00:43:14,835 | INFO | âœ… Model and tokenizer loaded successfully
2025-08-26 00:43:14,835 | INFO | ðŸ“Š Loaded 3 ARC problems
2025-08-26 00:43:14,851 | INFO | ðŸ“Š Created dataset with 3 examples
2025-08-26 00:43:22,015 | ERROR | âŒ Training error: GRPOTrainer.__init__() got an unexpected keyword argument 'generation_config'
Traceback (most recent call last):
  File "/home/ubuntu/gpt_oss_arc_final/main_hf_trl_dapo.py", line 577, in main_hf_trl_dapo
    trainer = GRPOTrainer(
TypeError: GRPOTrainer.__init__() got an unexpected keyword argument 'generation_config'
