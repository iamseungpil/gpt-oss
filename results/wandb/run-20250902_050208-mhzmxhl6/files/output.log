2025-09-02 05:02:09,354 | INFO | ðŸ“¦ Loading model: openai/gpt-oss-20b
`torch_dtype` is deprecated! Use `dtype` instead!
MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16
2025-09-02 05:02:13,185 | INFO | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards: 100%|â–ˆ| 3/3 [00:05<00:00,  1.78
trainable params: 7,962,624 || all params: 20,922,719,808 || trainable%: 0.0381
2025-09-02 05:02:20,442 | INFO | ðŸ“Š Preparing dataset...
2025-09-02 05:02:20,528 | INFO | ðŸ“Š Created dataset with 400 examples
[2025-09-02 05:02:21,122] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-09-02 05:02:21,167 | INFO | gcc -pthread -B /home/ubuntu/miniconda3/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/ubuntu/miniconda3/include -fPIC -O2 -isystem /home/ubuntu/miniconda3/include -fPIC -c /tmp/tmp30revl0q/test.c -o /tmp/tmp30revl0q/test.o
2025-09-02 05:02:21,181 | INFO | gcc -pthread -B /home/ubuntu/miniconda3/compiler_compat /tmp/tmp30revl0q/test.o -laio -o /tmp/tmp30revl0q/a.out
2025-09-02 05:02:21,334 | INFO | gcc -pthread -B /home/ubuntu/miniconda3/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/ubuntu/miniconda3/include -fPIC -O2 -isystem /home/ubuntu/miniconda3/include -fPIC -c /tmp/tmp40whq8q9/test.c -o /tmp/tmp40whq8q9/test.o
2025-09-02 05:02:21,347 | INFO | gcc -pthread -B /home/ubuntu/miniconda3/compiler_compat /tmp/tmp40whq8q9/test.o -L/usr/local/cuda-12.8 -L/usr/local/cuda-12.8/lib64 -lcufile -o /tmp/tmp40whq8q9/a.out
2025-09-02 05:02:22,096 | INFO | ðŸŽ¯ Starting training...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998, 'pad_token_id': 200002}.
  0%|                             | 0/200 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Caching is incompatible with gradient checkpointing in GptOssDecoderLayer. Setting `past_key_values=None`.
/home/ubuntu/miniconda3/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
