[13:43:51] [Rank 0] âœ… W&B initialized
[13:43:51] [Rank 0] ðŸ“¦ Loading GPT-OSS model and tokenizer...
[13:43:52] [Rank 0] âœ… Tokenizer loaded
[13:43:52] [Rank 0] ðŸ”„ Loading base model from HuggingFace: openai/gpt-oss-20b
`torch_dtype` is deprecated! Use `dtype` instead!
MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:27<00:00,  9.19s/it]
[13:44:21] [Rank 0] âœ… Base model loaded: 20,914,757,184 parameters
[13:44:21] [Rank 0] âœ… LoRA applied: 20,922,719,808 total, 7,962,624 trainable (0.0381%)
[13:44:21] [Rank 0] âœ… Loaded 10 training problems
[13:44:21] [Rank 0] ðŸ“ Output directory created: checkpoints_fsdp_fixed
[13:44:21] [Rank 0] ðŸš€ Initializing GRPO trainer with FSDP...
[13:44:21] [Rank 0] âœ… GRPO trainer initialized
[13:44:21] [Rank 0] ðŸ“Š Model parameters: 20,922,719,808
[13:44:21] [Rank 0] ðŸ“Š Training dataset size: 500
[13:44:21] [Rank 0] ðŸŽ¯ Max steps: 50
[13:44:21] [Rank 0] ================================================================================
[13:44:21] [Rank 0] ðŸš€ Starting training...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
  0%|          | 0/50 [00:00<?, ?it/s]
[13:44:35] [Rank 0] âŒ Training failed: expected mat1 and mat2 to have the same dtype, but got: float != c10::BFloat16
[13:44:35] [Rank 0] ðŸ“ Full traceback saved: checkpoints_fsdp_fixed/error_rank0_20250905_134435.log
