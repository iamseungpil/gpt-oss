2025-08-28 01:42:44,861 | INFO | Loading model and tokenizer...
Using MXFP4 quantized models requires a GPU, we will default to dequantizing the model to bf16
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:39<00:00, 13.14s/it]
trainable params: 7,962,624 || all params: 20,922,719,808 || trainable%: 0.0381
2025-08-28 01:44:04,147 | INFO | âœ… Model and tokenizer loaded successfully
2025-08-28 01:44:04,148 | INFO | ðŸ“Š Loaded 3 ARC problems
2025-08-28 01:44:05,274 | INFO | ðŸ“Š Created dataset with 3 examples
[2025-08-28 01:44:11,381] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-08-28 01:44:12,122 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -c /data/tmp/tmpk3b_sag8/test.c -o /data/tmp/tmpk3b_sag8/test.o
2025-08-28 01:44:12,197 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat /data/tmp/tmpk3b_sag8/test.o -laio -o /data/tmp/tmpk3b_sag8/a.out
2025-08-28 01:44:12,935 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -c /data/tmp/tmpdzbewxkx/test.c -o /data/tmp/tmpdzbewxkx/test.o
2025-08-28 01:44:12,964 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat /data/tmp/tmpdzbewxkx/test.o -L/usr/local/cuda-12.6 -L/usr/local/cuda-12.6/lib64 -lcufile -o /data/tmp/tmpdzbewxkx/a.out
2025-08-28 01:44:13,066 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -O2 -isystem /data/miniforge3/envs/gpt_oss_rl/include -fPIC -c /data/tmp/tmphld09vcz/test.c -o /data/tmp/tmphld09vcz/test.o
2025-08-28 01:44:13,097 | INFO | gcc -pthread -B /data/miniforge3/envs/gpt_oss_rl/compiler_compat /data/tmp/tmphld09vcz/test.o -laio -o /data/tmp/tmphld09vcz/a.out
[2025-08-28 01:44:14,823] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
2025-08-28 01:44:14,901 | INFO | ðŸš€ Starting HuggingFace TRL DAPO training...
2025-08-28 01:44:14,902 | INFO | ðŸ“ Using corrected Harmony format - no examples in input prompts
2025-08-28 01:44:14,902 | INFO |    Loss type: bnpo
2025-08-28 01:44:14,902 | INFO |    Epsilon: 0.2
2025-08-28 01:44:14,902 | INFO |    Epsilon high: 0.28
2025-08-28 01:44:14,903 | INFO |    Repetition penalty: 1.0
2025-08-28 01:44:14,903 | INFO |    Reward components: 4 (format + size + pixel + final_channel)
2025-08-28 01:44:15,252 | WARNING | Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 4. Using DeepSpeed's value.
Installed CUDA version 12.6 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/ubuntu/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
/data/miniforge3/envs/gpt_oss_rl/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.528813600540161 seconds
Parameter Offload - Persistent parameters statistics: param_count = 241, numel = 728640
  0%|          | 0/100 [00:00<?, ?it/s]
