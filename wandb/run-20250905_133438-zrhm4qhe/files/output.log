[13:34:39] [Rank 0] ✅ W&B initialized
[13:34:39] [Rank 0] 📦 Loading GPT-OSS model and tokenizer...
[13:34:41] [Rank 0] ✅ Tokenizer loaded
[13:34:41] [Rank 0] 🔄 Loading base model from HuggingFace: openai/gpt-oss-20b
`torch_dtype` is deprecated! Use `dtype` instead!
MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16
Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  9.31s/it]
[13:35:10] [Rank 0] ✅ Base model loaded: 20,914,757,184 parameters
[13:35:10] [Rank 0] ✅ LoRA applied: 20,922,719,808 total, 7,962,624 trainable (0.0381%)
[13:35:10] [Rank 0] 🔁 Cast model (incl. LoRA) to bfloat16
[13:35:23] [Rank 0] ✅ Model wrapped with FSDP
[13:35:23] [Rank 0] ✅ Loaded 10 training problems
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[13:35:23] [Rank 0] 📁 Output directory created: checkpoints_fsdp_fixed
[13:35:24] [Rank 0] 🚀 Initializing GRPO trainer with FSDP...
[13:35:24] [Rank 0] ✅ GRPO trainer initialized
[13:35:24] [Rank 0] 📊 Model parameters: 10,461,359,904
[13:35:24] [Rank 0] 📊 Training dataset size: 500
[13:35:24] [Rank 0] 🎯 Max steps: 50
[13:35:24] [Rank 0] ================================================================================
[13:35:24] [Rank 0] 🚀 Starting training...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
  0%|          | 0/50 [00:00<?, ?it/s]
[13:35:24] [Rank 0] ❌ Training failed: 'weight' must be 2-D
