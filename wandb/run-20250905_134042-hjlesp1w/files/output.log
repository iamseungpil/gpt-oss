[13:40:43] [Rank 0] âœ… W&B initialized
[13:40:43] [Rank 0] ðŸ“¦ Loading GPT-OSS model and tokenizer...
[13:40:45] [Rank 0] âœ… Tokenizer loaded
[13:40:45] [Rank 0] ðŸ”„ Loading base model from HuggingFace: openai/gpt-oss-20b
`torch_dtype` is deprecated! Use `dtype` instead!
MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:27<00:00,  9.15s/it]
[13:41:13] [Rank 0] âœ… Base model loaded: 20,914,757,184 parameters
[13:41:13] [Rank 0] âœ… LoRA applied: 20,922,719,808 total, 7,962,624 trainable (0.0381%)
[13:41:13] [Rank 0] ðŸ” Cast model (incl. LoRA) to bfloat16
[13:41:13] [Rank 0] âœ… Loaded 10 training problems
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[13:41:14] [Rank 0] ðŸ“ Output directory created: checkpoints_fsdp_fixed
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[13:41:14] [Rank 0] ðŸš€ Initializing GRPO trainer with FSDP...
[13:41:14] [Rank 0] âŒ Training failed: The activation_checkpointing in FSDP config and the gradient_checkpointing in training arg can't be set to True simultaneously. Please use FSDP's activation_checkpointing logic when using FSDP.
